# Distributed File Creation System - Architecture Documentation

## üìã Table of Contents

- [System Overview](#system-overview)
- [Architecture Diagram](#architecture-diagram)
- [Technology Stack](#technology-stack)
- [Service Components](#service-components)
- [Data Flow](#data-flow)
- [Kafka Configuration](#kafka-configuration)
- [Scaling Strategy](#scaling-strategy)
- [Fault Tolerance](#fault-tolerance)
- [Monitoring & Progress Tracking](#monitoring--progress-tracking)
- [Deployment](#deployment)

---

## System Overview

A distributed, event-driven system for asynchronous file creation and processing. Users submit file creation requests via a web UI, which are processed by a scalable backend service using Kafka for message queuing and S3 for storage.

### Key Features

- ‚úÖ **Asynchronous Processing**: Non-blocking file creation via Kafka messaging
- ‚úÖ **Horizontal Scalability**: Multiple consumer instances processing in parallel
- ‚úÖ **Fault Tolerance**: Manual offset commit prevents message loss on failures
- ‚úÖ **Long-Running Tasks**: Heartbeat mechanism prevents session timeouts
- ‚úÖ **Real-time Progress**: UI polls for status updates every 2 seconds
- ‚úÖ **Partitioned Processing**: 3 Kafka partitions for concurrent workload distribution

---

## Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER INTERFACE                          ‚îÇ
‚îÇ                   (Next.js + React + Tailwind)                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Create     ‚îÇ  ‚îÇ   View       ‚îÇ  ‚îÇ   Download   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Files      ‚îÇ  ‚îÇ   Status     ‚îÇ  ‚îÇ   Files      ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ HTTP REST API
                             ‚îÇ (Polling every 2s)
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        API SERVICE                              ‚îÇ
‚îÇ                    (NestJS + TypeORM)                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ   REST      ‚îÇ    ‚îÇ  Kafka      ‚îÇ    ‚îÇ PostgreSQL  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   API       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Producer   ‚îÇ    ‚îÇ  Database   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Consumer   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Status     ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                             ‚îÇ                   ‚ñ≤               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ                   ‚îÇ
                    file.create topic    file.created topic
                              ‚îÇ                   ‚îÇ
                              ‚Üì                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      KAFKA CLUSTER                              ‚îÇ
‚îÇ                      (KRaft Mode)                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Topic: file.create             Topic: file.created            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Partition 0  ‚îÇ  Partition 1  ‚îÇ  Partition 2       ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Round-robin distribution across 3 partitions      ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ Consumer Group: file-service-group
                             ‚îÇ (2 instances)
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FILE SERVICE                                 ‚îÇ
‚îÇ                 (NestJS + KafkaJS + AWS SDK)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Instance 1          ‚îÇ       ‚îÇ  Instance 2          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Partition 2         ‚îÇ       ‚îÇ  Partitions 0, 1     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ       ‚îÇ                      ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ       ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Kafka Consumer ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ Kafka Consumer ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ       ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ          ‚îÇ       ‚îÇ          ‚îÇ          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ       ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  File          ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ  File          ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îÇ  Processor     ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ  Processor     ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ       ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ          ‚îÇ       ‚îÇ          ‚îÇ          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ       ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îÇ S3 Upload      ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ S3 Upload      ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ       ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ          ‚îÇ       ‚îÇ          ‚îÇ          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ       ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Kafka Producer ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ Kafka Producer ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ       ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   LOCALSTACK S3                                 ‚îÇ
‚îÇ                 (Local S3-compatible storage)                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Bucket: file-creation-bucket                                  ‚îÇ
‚îÇ  Files: file-{id}-{timestamp}.txt                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   POSTGRESQL DATABASE                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Table: files                                                  ‚îÇ
‚îÇ  Columns: id, title, description, status,                      ‚îÇ
‚îÇ           s3_location, created_at, updated_at                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Technology Stack

### Frontend

- **Next.js 14**: React framework with App Router
- **TypeScript**: Type-safe development
- **Tailwind CSS**: Utility-first styling
- **HTTP Polling**: Status updates every 2 seconds

### Backend Services

#### API Service

- **NestJS**: Node.js framework
- **TypeORM**: Database ORM
- **PostgreSQL**: Relational database
- **KafkaJS**: Kafka client library
- **Express**: HTTP server

#### File Service

- **NestJS**: Node.js framework
- **KafkaJS**: Kafka consumer/producer
- **AWS SDK v3**: S3 operations
- **LocalStack**: Local AWS emulation

### Infrastructure

- **Apache Kafka 7.8.0**: Message broker (KRaft mode)
- **PostgreSQL 16**: Persistent storage
- **LocalStack**: S3-compatible object storage
- **Docker Compose**: Container orchestration

---

## Service Components

### 1. API Service (`api-service/`)

**Port**: `3006`

**Responsibilities**:

- Expose REST API for file operations
- Create database records for new files
- Produce messages to Kafka (`file.create` topic)
- Consume status updates from Kafka (`file.created` topic)
- Update database with processing results
- Serve file downloads from S3

**Key Files**:

- `src/files/files.controller.ts` - REST endpoints
- `src/files/files.service.ts` - Business logic
- `src/kafka/kafka.service.ts` - Kafka producer/consumer
- `src/files/entities/file.entity.ts` - Database schema

**Database Schema**:

```typescript
{
  id: number; // Auto-increment primary key
  title: string; // File title (255 chars)
  description: string; // Optional description (text)
  status: string; // pending | processing | completed | failed
  s3_location: string; // S3 path (nullable)
  created_at: Date; // Auto-generated
  updated_at: Date; // Auto-updated
}
```

### 2. File Service (`file-service/`)

**Port**: `3001` (Instance 1), `3002` (Instance 2)

**Responsibilities**:

- Consume file creation requests from Kafka
- Generate file content
- Upload files to S3
- Send heartbeats during long processing
- Produce completion/failure events to Kafka
- Handle failures gracefully (no offset commit on error)

**Key Files**:

- `src/processor/file-processor.service.ts` - File processing logic
- `src/kafka/kafka.service.ts` - Kafka consumer/producer with manual commit
- `src/s3/s3.service.ts` - S3 operations

**Processing Flow**:

```typescript
1. Receive message from Kafka (file.create topic)
2. Parse fileId, title, description
3. Simulate 1-minute processing (6 √ó 10-second intervals)
4. Send heartbeat every 10 seconds (prevents timeout)
5. Generate file content
6. Upload to S3 ‚Üí Get s3Location
7. Produce success event to file.created topic
8. Commit Kafka offset (message successfully processed)

On Error:
- Log error
- DON'T commit offset
- Message will be redelivered on restart
```

### 3. UI Service (`file-creation-ui/`)

**Port**: `3004`

**Responsibilities**:

- Render file creation form
- Display file list with status
- Auto-refresh status every 2 seconds
- Provide download links for completed files
- Show system statistics (pending/processing/completed counts)

**Key Features**:

- Auto-refresh toggle
- Real-time status updates via polling
- Color-coded status indicators
- Download button for completed files

---

## Data Flow

### Complete Request Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: User Creates File                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User enters title + description ‚Üí Clicks "Create File"
                 ‚Üì
        POST /files HTTP request
                 ‚Üì
        API Service receives request

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: API Service Processes Request                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ë† Create database record:
   INSERT INTO files (title, description, status)
   VALUES ('My File', 'Description', 'pending')
   ‚Üí Returns fileId: 42

‚ë° Produce Kafka message:
   Topic: file.create
   Partition: Round-robin (0, 1, or 2)
   Message: {
     fileId: 42,
     title: 'My File',
     description: 'Description',
     timestamp: '2026-01-04T12:00:00Z'
   }

‚ë¢ Return response to UI:
   { id: 42, title: 'My File', status: 'pending', ... }

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: Kafka Distributes Message                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Kafka receives message ‚Üí Assigns to partition (round-robin)

Example: Message lands on Partition 1
         ‚Üì
Consumer Group: file-service-group
         ‚Üì
Instance 2 handles Partition 1 ‚Üí Picks up message

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 4: File Service Processes                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Instance 2, Partition 1, Offset 15:

üì® Received message: { fileId: 42, ... }

üïê Start 1-minute processing:
   00:00 - Start processing
   00:10 - ‚ù§Ô∏è Heartbeat sent (prevents timeout)
   00:20 - ‚ù§Ô∏è Heartbeat sent
   00:30 - ‚ù§Ô∏è Heartbeat sent
   00:40 - ‚ù§Ô∏è Heartbeat sent
   00:50 - ‚ù§Ô∏è Heartbeat sent
   01:00 - ‚úÖ Processing complete

üìù Generate file content:
   ===================
   GENERATED FILE
   ===================
   Title: My File
   Description: Description
   Generated at: 2026-01-04T12:01:00Z

‚òÅÔ∏è Upload to S3:
   Bucket: file-creation-bucket
   Key: file-42-1704369660000.txt
   ‚Üí s3Location: "file-42-1704369660000.txt"

üì§ Produce success event:
   Topic: file.created
   Message: {
     fileId: 42,
     s3Location: "file-42-1704369660000.txt",
     status: "completed",
     timestamp: '2026-01-04T12:01:00Z'
   }

‚úÖ Commit Kafka offset:
   Partition: 1, Offset: 16 (15 + 1)
   ‚Üí Message marked as processed

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: API Service Updates Status                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

API Service consumes from file.created topic:

‚úì Received: { fileId: 42, status: "completed", ... }

UPDATE files
SET status = 'completed',
    s3_location = 'file-42-1704369660000.txt',
    updated_at = NOW()
WHERE id = 42

‚úì Database updated

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 6: UI Shows Updated Status                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

UI polls every 2 seconds:
   GET /files ‚Üí Fetches all files from database

File object returned:
{
  id: 42,
  title: 'My File',
  status: 'completed',     ‚Üê Updated from 'pending'
  s3_location: 'file-42-1704369660000.txt',
  created_at: '2026-01-04T12:00:00Z',
  updated_at: '2026-01-04T12:01:00Z'
}

UI updates display:
- Status badge: Yellow ‚Üí Green
- Download button appears
- System stats: Completed count +1
```

---

## Kafka Configuration

### Topics

#### `file.create`

- **Partitions**: 3
- **Replication Factor**: 1
- **Purpose**: File creation requests from API ‚Üí File Service
- **Message Key**: `null` (enables round-robin distribution)
- **Retention**: Default (7 days)

#### `file.created`

- **Partitions**: 3
- **Replication Factor**: 1
- **Purpose**: Processing results from File Service ‚Üí API
- **Message Key**: `null`
- **Retention**: Default (7 days)

### Consumer Configuration

```typescript
// file-service/src/kafka/kafka.service.ts

consumer = kafka.consumer({
  groupId: "file-service-group", // All instances in same group
});

await consumer.run({
  autoCommit: false, // CRITICAL: Manual commit only

  eachMessage: async (payload) => {
    try {
      await handler(payload, payload.heartbeat);

      // Only commit offset after successful processing
      await consumer.commitOffsets([
        {
          topic: payload.topic,
          partition: payload.partition,
          offset: (parseInt(payload.message.offset) + 1).toString(),
        },
      ]);
    } catch (error) {
      // DON'T commit - Kafka will redeliver on restart
      throw error;
    }
  },
});
```

### Partition Distribution

**Round-Robin Strategy** (Key = `null`):

```
Message 1 ‚Üí Partition 0
Message 2 ‚Üí Partition 1
Message 3 ‚Üí Partition 2
Message 4 ‚Üí Partition 0  (cycles back)
Message 5 ‚Üí Partition 1
Message 6 ‚Üí Partition 2
...
```

**Consumer Assignment** (2 instances):

```
Instance 1: Partition 2
Instance 2: Partitions 0, 1

Total Capacity: 3 messages in parallel
```

### Heartbeat Mechanism

**Problem**: Long-running tasks (1 minute) exceed default session timeout (30s)

**Solution**: Send heartbeats every 10 seconds

```typescript
// During 1-minute processing
for (let i = 0; i < 6; i++) {
  await new Promise((resolve) => setTimeout(resolve, 10000)); // 10s
  await heartbeat(); // Tells Kafka: "I'm still alive!"
  console.log(`‚ù§Ô∏è Heartbeat sent (${(i + 1) * 10}s elapsed)`);
}
```

**Without Heartbeat**:

```
00:00 - Start processing
00:30 - Session timeout ‚Üí Kafka thinks consumer died
00:30 - Rebalance triggered ‚Üí Message reassigned
00:30 - Another consumer picks up same message
Result: Duplicate processing
```

**With Heartbeat**:

```
00:00 - Start processing
00:10 - ‚ù§Ô∏è Heartbeat ‚Üí Session renewed
00:20 - ‚ù§Ô∏è Heartbeat ‚Üí Session renewed
00:30 - ‚ù§Ô∏è Heartbeat ‚Üí Session renewed (would have timed out)
00:40 - ‚ù§Ô∏è Heartbeat ‚Üí Session renewed
00:50 - ‚ù§Ô∏è Heartbeat ‚Üí Session renewed
01:00 - ‚úÖ Processing complete ‚Üí Commit offset
Result: No rebalance, clean processing
```

---

## Scaling Strategy

### Horizontal Scaling

**Current Setup**: 2 File Service instances

```bash
# docker-compose.yaml
file-service:
  deploy:
    replicas: 2  # Scale to 2 instances
```

**How Kafka Distributes Partitions**:

| Consumers    | Partition Assignment                                     |
| ------------ | -------------------------------------------------------- |
| 1 instance   | Instance 1: [0, 1, 2]                                    |
| 2 instances  | Instance 1: [2]<br>Instance 2: [0, 1]                    |
| 3 instances  | Instance 1: [0]<br>Instance 2: [1]<br>Instance 3: [2]    |
| 4+ instances | Max 3 active (one per partition)<br>Others idle (backup) |

**Maximum Concurrency**: 3 partitions = 3 messages processed in parallel

### Increasing Throughput

**Option 1: More Partitions** (Recommended)

```bash
# Update to 6 partitions
docker exec kafka kafka-topics --alter \
  --topic file.create \
  --partitions 6

# Scale to 6 instances
docker compose up -d --scale file-service=6
```

**Result**: 6 messages processed concurrently

**Option 2: Faster Processing**

- Remove artificial 1-minute delay
- Optimize S3 uploads
- Use multipart uploads for large files

**Option 3: Multiple Consumer Groups**

- Create separate consumer group for specific workloads
- E.g., `file-service-group-high-priority` for urgent files

### Vertical Scaling

```yaml
file-service:
  deploy:
    resources:
      limits:
        cpus: "2.0"
        memory: 2G
```

---

## Fault Tolerance

### Message Loss Prevention

**Strategy**: Manual offset commit only after successful processing

```typescript
‚úÖ Success Path:
   Process message ‚Üí Upload to S3 ‚Üí Produce result ‚Üí Commit offset

‚ùå Failure Path:
   Process message ‚Üí Error ‚Üí DON'T commit offset ‚Üí Message redelivered
```

### Failure Scenarios

#### Scenario 1: File Service Crashes During Processing

```
State: Processing fileId 42, offset 15
       Uploaded to S3 ‚úì
       About to produce success event...
       üí• CRASH

Kafka State: Offset 15 NOT committed

On Restart:
- Kafka redelivers message at offset 15
- File processor runs again
- S3 upload happens again (overwrites previous)
- Success event produced
- Offset committed

Result: ‚úÖ No data loss (at-least-once delivery)
```

#### Scenario 2: S3 Upload Fails

```
Error: S3 bucket not accessible

Response:
- Catch error
- Log: "‚ùå Error processing file 42"
- DON'T commit offset
- throw error

Result:
- Kafka marks message as unprocessed
- On restart, message redelivered
- Retry processing
```

#### Scenario 3: Kafka Producer Fails

```
State: File uploaded to S3 ‚úì
       Producing to file.created topic... üí• NETWORK ERROR

Response:
- Exception thrown
- Offset NOT committed

Result:
- Message redelivered on restart
- File re-uploaded (S3 overwrite)
- Success event produced again
- Idempotent (same outcome)
```

### Rebalancing

**When Rebalance Happens**:

- New consumer joins the group
- Consumer leaves/crashes
- Partitions added to topic
- Consumer heartbeat timeout

**During Rebalance**:

```
1. Kafka pauses consumption
2. Reassigns partitions among consumers
3. Each consumer seeks to last committed offset
4. Consumption resumes

Example:
Before: Instance 1 [0,1,2]
        Instance 2 [none] (joins)
After:  Instance 1 [2]
        Instance 2 [0,1]
```

### Data Consistency

**Database State**:

- Status updated via Kafka events (eventually consistent)
- UI shows last committed state (may be slightly stale)
- 2-second polling ensures UI catches up quickly

**S3 State**:

- Files uploaded with unique keys: `file-{id}-{timestamp}.txt`
- Overwrite on retry (same fileId)
- No orphaned files

---

## Monitoring & Progress Tracking

### Status Flow

```
Database Status Transitions:

pending ‚Üí completed   (Success path)
pending ‚Üí failed      (Error path)

Note: "processing" status exists but not actively used
      (Could be set via separate Kafka event if needed)
```

### UI Polling Mechanism

```typescript
// Poll every 2 seconds
useEffect(() => {
  const interval = setInterval(() => {
    fetch("/files").then((data) => setFiles(data));
  }, 2000);
  return () => clearInterval(interval);
}, []);
```

**Trade-offs**:

| Approach                     | Pros                                      | Cons                            |
| ---------------------------- | ----------------------------------------- | ------------------------------- |
| **HTTP Polling (Current)**   | Simple, stateless, load balancer friendly | 2s delay, more HTTP requests    |
| **WebSockets**               | Real-time, efficient                      | Complex, sticky sessions needed |
| **Server-Sent Events (SSE)** | Real-time, simpler than WS                | Unidirectional only             |

### Logging

**API Service Logs**:

```
‚úì Received file creation request: { fileId: 42, ... }
‚úì Produced 1 message(s) to topic: file.create
‚úì Received file creation status: { fileId: 42, status: 'completed' }
‚úì Updated file 42 status to completed
```

**File Service Logs**:

```
üì® Received message from partition 1, offset 15
KAFKA: ‚úì Received file creation request: { fileId: 42, ... }
üïê Starting 1-minute processing for file 42...
‚ù§Ô∏è  Heartbeat sent for file 42 (10s elapsed)
‚ù§Ô∏è  Heartbeat sent for file 42 (20s elapsed)
...
‚úÖ 1-minute processing complete for file 42
‚úì Produced 1 message(s) to topic: file.created
‚úÖ Committed offset 15 for partition 1
‚úì Successfully processed file 42
```

### Metrics to Monitor

1. **Kafka Consumer Lag**: Difference between produced and consumed offsets
2. **Processing Time**: Time from message received to offset committed
3. **Error Rate**: Failed messages / total messages
4. **Partition Balance**: Message distribution across partitions
5. **S3 Upload Success Rate**: Successful uploads / total attempts

---

## Deployment

### Prerequisites

- Docker & Docker Compose
- Node.js 20+ (for local development)
- 8GB RAM minimum

### Quick Start

```bash
# Clone repository
git clone <repo-url>
cd kafkajs

# Start all services
./start.sh

# Services will be available at:
# - UI:        http://localhost:3004
# - API:       http://localhost:3006
# - Kafka:     localhost:29092
# - PostgreSQL: localhost:5432
```

### Manual Setup

```bash
# 1. Start infrastructure
docker compose up -d kafka postgres-db localstack-s3

# 2. Create Kafka topics
docker exec kafka kafka-topics --bootstrap-server kafka:9092 \
  --create --topic file.create --partitions 3 --replication-factor 1

docker exec kafka kafka-topics --bootstrap-server kafka:9092 \
  --create --topic file.created --partitions 3 --replication-factor 1

# 3. Run database migrations
cd api-service
npm install
npm run migration:run

# 4. Build and start services
docker compose up -d --build

# 5. Scale file-service to 2 instances
docker compose up -d --scale file-service=2
```

### Rebuilding After Code Changes

```bash
# Rebuild specific service
./rebuild.sh file-service

# Rebuild multiple services
./rebuild.sh api-service file-service

# Rebuild all
docker compose up -d --build
```

### Environment Variables

**API Service** (`.env`):

```bash
PORT=3006
DATABASE_HOST=postgres-db
DATABASE_PORT=5432
DATABASE_USER=fileuser
DATABASE_PASSWORD=filepass
DATABASE_NAME=filedb
KAFKA_BROKER=kafka:9092
S3_ENDPOINT=http://localstack-s3:4566
S3_REGION=us-east-1
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test
```

**File Service** (`.env`):

```bash
KAFKA_BROKER=kafka:9092
S3_ENDPOINT=http://localstack-s3:4566
S3_REGION=us-east-1
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test
```

**UI** (`.env.local`):

```bash
NEXT_PUBLIC_API_URL=http://localhost:3006
```

### Health Checks

```bash
# Check all services
docker compose ps

# Check Kafka topics
docker exec kafka kafka-topics --list --bootstrap-server kafka:9092

# Check consumer groups
docker exec kafka kafka-consumer-groups --list --bootstrap-server kafka:9092

# Check file-service instances
docker compose ps file-service

# View logs
docker compose logs -f file-service
docker compose logs -f api-service
```

---

## Useful Commands

### Kafka Operations

```bash
# List topics
docker exec kafka kafka-topics --list --bootstrap-server kafka:9092

# Describe topic
docker exec kafka kafka-topics --describe --topic file.create --bootstrap-server kafka:9092

# Consume messages
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic file.create \
  --from-beginning

# Check consumer lag
docker exec kafka kafka-consumer-groups \
  --bootstrap-server kafka:9092 \
  --describe \
  --group file-service-group

# Alter partitions (scale up only)
docker exec kafka kafka-topics --alter \
  --topic file.create \
  --partitions 6 \
  --bootstrap-server kafka:9092
```

### Database Operations

```bash
# Connect to PostgreSQL
docker exec -it postgres-db psql -U fileuser -d filedb

# Check files table
SELECT id, title, status, s3_location FROM files ORDER BY created_at DESC;

# Count by status
SELECT status, COUNT(*) FROM files GROUP BY status;
```

### S3 Operations

```bash
# List buckets
docker exec localstack-s3 awslocal s3 ls

# List files in bucket
docker exec localstack-s3 awslocal s3 ls s3://file-creation-bucket/

# Download file
docker exec localstack-s3 awslocal s3 cp \
  s3://file-creation-bucket/file-42-1704369660000.txt \
  /tmp/file.txt
```

### Service Management

```bash
# Scale file-service
docker compose up -d --scale file-service=3

# Restart service
docker compose restart file-service

# View logs (follow)
docker compose logs -f file-service

# Stop all services
docker compose down

# Remove volumes (clean slate)
docker compose down -v
```

---

## Performance Characteristics

### Throughput

**Current Setup** (3 partitions, 2 instances):

- **Parallel Processing**: 3 files simultaneously
- **Processing Time**: 1 minute per file (artificial delay)
- **Throughput**: ~3 files/minute

**Without Artificial Delay**:

- **Processing Time**: ~100-500ms per file
- **Throughput**: ~360-1800 files/minute

### Latency

- **API Response**: < 50ms (just creates DB record + Kafka message)
- **End-to-End**: ~1 minute (current), ~1-2 seconds (without delay)
- **UI Update**: 0-2 seconds (polling interval)

### Scalability Limits

| Component              | Current | Max Recommended |
| ---------------------- | ------- | --------------- |
| Partitions             | 3       | 10-20           |
| File Service Instances | 2       | = Partitions    |
| Concurrent Processing  | 3       | = Partitions    |
| UI Polling Clients     | N/A     | 1000+           |
| Database Connections   | 10      | 100             |

---

## Future Enhancements

### Short Term

- [ ] Add "processing" status tracking via separate Kafka event
- [ ] Implement actual retry logic with exponential backoff
- [ ] Add metrics/monitoring (Prometheus + Grafana)
- [ ] WebSocket support for real-time status updates
- [ ] File upload from UI (not just generation)

### Medium Term

- [ ] Dead Letter Queue (DLQ) for failed messages
- [ ] Distributed tracing (OpenTelemetry)
- [ ] Rate limiting on API endpoints
- [ ] User authentication & authorization
- [ ] File metadata (size, MIME type, tags)

### Long Term

- [ ] Multi-region deployment
- [ ] CDC (Change Data Capture) from PostgreSQL
- [ ] Event sourcing for audit trail
- [ ] GraphQL API alongside REST
- [ ] Kubernetes deployment with Helm charts

---

## Troubleshooting

### Consumer Not Receiving Messages

```bash
# Check consumer group status
docker exec kafka kafka-consumer-groups \
  --bootstrap-server kafka:9092 \
  --describe \
  --group file-service-group

# Look for:
# - LAG column (should be 0 or increasing slowly)
# - STATE column (should be "Stable")
```

### Rebalancing Loop

**Symptom**: Consumer keeps rejoining group

**Causes**:

- Session timeout during long processing ‚Üí Fixed by heartbeats
- Consumer taking too long to poll ‚Üí Increase `sessionTimeout`
- Network issues ‚Üí Check Kafka connectivity

### Message Stuck in Partition

**Symptom**: Message not being processed

**Check**:

```bash
# View consumer lag
docker exec kafka kafka-consumer-groups \
  --bootstrap-server kafka:9092 \
  --describe \
  --group file-service-group

# If LAG > 0 and not decreasing:
# - Consumer might be crashed
# - Processing failed and offset not committed
```

**Solution**:

- Restart file-service: `docker compose restart file-service`
- Check logs for errors: `docker compose logs file-service`

### Status Not Updating in UI

**Check**:

1. File Service processed successfully? ‚Üí Check logs
2. Success event produced? ‚Üí `docker exec kafka kafka-console-consumer ...`
3. API Service consumed event? ‚Üí Check API logs
4. Database updated? ‚Üí `SELECT * FROM files WHERE id = X`
5. UI polling working? ‚Üí Check browser network tab

---

## License

MIT

## Contributors

[Your Name]

## Last Updated

January 4, 2026
